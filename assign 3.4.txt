1) HDFS federation and High availability



HDFS Fedration- 
                Namenode is responsible for the successful operation of HDFS
               Namenode holds the entire metadata of HDFS, which includes information about files and directories like who created the file when it was created, when it was last modified, permissions  
               Namnode holds the locations of the blocks that make up each file in HDFS in its memory
               All clients has to go through Namenode to perform any read and Write operation in HDFS
               Since Namenode has the entire metadata of HDFS in a big cluster Namenode can become huge in volume its memory becomes a limiting factor and will start to slow down
 	       In Hadoop 1.x there is only one NameNode allow only one Active NameNode in a cluster, which maintains a single namespace
               when there are more data nodes with many files the NameNode memory will reach its limit and it becomes the limiting factor for cluster scaling


HDFS availablity-
                  The HDFS NameNode High Availability feature enables you to run redundant NameNodes in the same cluster 
                 This eliminates the NameNode as a potential single point of failure in an HDFS cluster
	         if a cluster had a single NameNode and that machine or process became unavailable the entire cluster would be unavailable until the NameNode was either restarted or started on a separate machine
                 This situation impacted the total availability of the HDFS cluster in two major ways
                  * if an unplanned event such as a machine crash, the cluster would be unavailable until an operator restarted the NameNode
                  *Planned maintenance events such as software or hardware upgrades on the NameNode machine would result in periods of cluster downtime
	          HDFS NameNode High Availablity avoids this by facilitating either a fast failover to the new NameNode during machine crash or a graceful administrator-initiated failover during planned maintenance





===============================================================================================================================================================================================================================

2)HDFS handles failures while writing data



HDFS handls failures while writing data:

 		*The pipeline is closed and any packets in the ack queue are added to the front of the data queue
		*The current block on the good DataNodes is given a new identity, which is communicated to the NameNode
		*The failed DataNode is removed from the pipeline, and a new pipeline is constructed from the two good DataNodes
		*The remainder of the block’s data is written to the good DataNodes in the pipeline
		*The NameNode notices that the block is under-replicated, and it arranges for a further
		replica to be created on another node
		*As long as dfs.namenode.replication.min replicas which defaults to 1 are written thewrite will succeed
		*The block will be asynchronously replicated across the cluster until its target replicationfactor is reached 
   